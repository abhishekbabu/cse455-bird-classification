<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>CSE 455 - Rice Image Classification</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.ico" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/animate.css/animate.min.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Mentor - v4.7.0
  * Template URL: https://bootstrapmade.com/mentor-free-education-bootstrap-theme/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top">
    <div class="container d-flex align-items-center">

      <h1 class="logo me-auto"><a href="https://courses.cs.washington.edu/courses/cse455/22sp/" target="_blank">CSE 455 (SP '22)</a></h1>

      <nav id="navbar" class="navbar order-last order-lg-0">
        <ul>
          <li><a class="active" href="#">Home</a></li>
          <li><a href="#description">Description</a></li>
          <li><a href="#dataset">Dataset</a></li>
          <li><a href="#experiments">Experiments</a></li>
          <li><a href="#results">Results</a></li>
          <li><a href="#discussion">Discussion</a></li>
          <li><a href="https://github.com/abhishekbabu/cse455-rice-classification" target="_blank">Code</a></li>
          <li><a href="https://youtu.be/DBiEtzIXmxc" target="_blank">Video</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero" class="d-flex justify-content-center align-items-center">
    <div class="container position-relative" data-aos="zoom-in" data-aos-delay="100">
      <h1>Rice Image<br>Classification</h1>
      <h2><a href="https://abhishekbabu.github.io/" target="_blank">Abhishek Babu</a>, <a href="https://www.linkedin.com/in/phdn/" target="_blank">Phuong Nguyen</a></h2>
    </div>
  </section><!-- End Hero -->

  <main id="main">

    <section id="description" class="about">
      <div class="container" data-aos="fade-up" style="text-align: center;">

        <iframe width="560" height="315" src="https://www.youtube.com/embed/DBiEtzIXmxc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

      </div>
    </section>

    <!-- ======= About Section ======= -->
    <section id="description" class="about">
      <div class="container" data-aos="fade-up">

        <div class="row">

          <div class="col-lg-6 order-1 order-lg-2" data-aos="fade-left" data-aos-delay="100">
            <img src="assets/img/about.jpg" class="img-fluid" alt="">
          </div>
          <div class="col-lg-6 pt-4 pt-lg-0 order-2 order-lg-1 content">
            <h2>Description</h2>
            <p>
              Rice is the most widely consumed staple food for over half of the world's population and is one of the most produced agricultural commodities worldwide. Rice consists of numerous genetic varieties.
              These varieties are separated from each other due to some of their features such a texture, shape, color. Using these visual features, it is possible to classify and evaluate the quality of rice grains.

              Using the <a href="https://www.kaggle.com/datasets/muratkokludataset/rice-image-dataset" target="_blank">Rice Image Dataset</a> from Kaggle, we aim to explore various transfer learning approaches with
              neural network models to see how they generalize to classifying rice varieties. The specific neural network models that we apply transfer learning (suing their PyTorch implementations) to are:
            </p>
            <ul>
              <li><i class="bi bi-check-circle"></i> <a href="https://arxiv.org/abs/1404.5997" target="_blank">AlexNet</a> (a classic model)</li>
              <li><i class="bi bi-check-circle"></i> <a href="https://arxiv.org/abs/1512.03385" target="_blank">ResNet18</a> (a classic model)</li>
              <li><i class="bi bi-check-circle"></i> <a href="https://arxiv.org/abs/1602.07360" target="_blank">SqueezeNet</a> (a more recent-ish model)</li>
            </ul>
            <p>
              We also use three different optimization strategies (<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank">SGD</a>, <a href="https://arxiv.org/abs/1212.5701" target="_blank">Adadelta</a>,
              <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank">RMSprop</a>) for each model to see how they impact model performance for transfer learning.
            </p>

          </div>
        </div>

      </div>
    </section>

    <section id="dataset" class="about">
      <div class="container">

        <div class="row">
          <div class="col-lg-12 content">
            <h2>Dataset</h2>
            <p>
              The <a href="https://www.kaggle.com/datasets/muratkokludataset/rice-image-dataset" target="_blank">Rice Image Dataset</a> from Kaggle is provided by <a href="https://www.muratkoklu.com/en/home/" target=""_blank>Murat Koklu</a>.
              The dataset consists of 75,000 images of 5 rice varieties (often grown in Turkey):
            </p>
            <ul>
              <li><i class="bi bi-check-circle"></i> <a href="https://en.wikipedia.org/wiki/Arborio_rice" target="_blank">Arborio</a></li>
              <li><i class="bi bi-check-circle"></i> <a href="https://en.wikipedia.org/wiki/Basmati" target="_blank">Basmati</a></li>
              <li><i class="bi bi-check-circle"></i> Ipsala</li>
              <li><i class="bi bi-check-circle"></i> <a href="https://en.wikipedia.org/wiki/Jasmine_rice" target="_blank">Jasmine</a></li>
              <li><i class="bi bi-check-circle"></i> Karacadag</li>
            </ul>
            <p>
              There are 15,000 images for each variety in the dataset. Each image consists of a single grain of rice of the appropriate variety. Here are some examples of images from the dataset:
            </p>

            <p style="text-align:center;"><img src="assets/img/examples.png" class="img-fluid" alt=""></p>

            <br/>

            <h3>Preprocessing</h3>
            <p>
              The data was pre-labeled appropriately, and so, we did not have to manually label or re-label the data. When loading in the images, we resize them to 256x256x3 and then take a center crop of the image of size 224x224x3, which is the
              input size for all three neural network models being trained. Then, we normalize the images. We then split the data into training, validation and testing sets using an 80:10:10 split (60000 training images, 10000 validation images,
              and 10000 testing images). The code for data preprocessing (and the rest of the code for the experiments) can be found in our <a href="https://github.com/abhishekbabu/cse455-rice-classification" target="_blank">GitHub repository</a>.
            </p>

          </div>
        </div>

      </div>
    </section>

    <section id="experiments" class="about">
      <div class="container">

        <div class="row">
          <div class="col-lg-12 content">
            <h2>Experiments</h2>
            <p>
              The three neural network models we apply transfer learning to were all trained on the <a href="https://www.image-net.org/" target="_blank">ImageNet</a> dataset. We experiment to see how well the "knowledge" learned by these models
              on the ImageNet dataset can be translated to classify these rice varieties. The models' <a href="https://pytorch.org/" target="_blank">PyTorch</a> implementations are loaded using the <a href="https://pytorch.org/vision/stable/index.html" target="_blank">Torchvision</a>
              <a href="https://pytorch.org/vision/0.8/models.html" target="_blank">models</a> subpackage. The optimizers' PyTorch implementations are loaded using the Torch <a href="https://pytorch.org/docs/stable/optim.html" target="_blank">optim</a> subpackage. We use
              <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" target="_blank">cross entropy loss</a> as the loss criterion during training.
            </p>

            <p>
              The models are trained for 25 epochs. After each training epoch, the accuracy of the model on the validation set is computed. The epoch at which the model exhibits the highest validation accuracy is used to save the best-performing model. After training,
              the accuracy of the model on the testing set is computed.
            </p>

            <br/>

            <h3>Models</h3>

            <p>
              For all the models, since we want to take advantage of what was already learned by training on the ImageNet dataset, we load pretrained versions of the models, but we only finetune the weights on the output layers that we change to match the dimensions
              of the number of classes in our dataset. The architectures for each model are available in the source code linked in the title of each model below.
            </p>

            <h4><a href="https://pytorch.org/vision/0.12/_modules/torchvision/models/alexnet.html" target="_blank">AlexNet</a></h4>
            
            <p>
              AlexNet is a landmark model based on CNN architecture. It won the ImageNet large-scale visual recognition challenge in 2012. The model was proposed by <a href="https://en.wikipedia.org/wiki/Alex_Krizhevsky" target="_blank">Alex Krizhevsky</a> and his colleagues.
              We load the pretrained version of this model and then replace the fully connected output layer with a newly initialized fully connected layer that has 5 output nodes (for the 5 rice varieties in the dataset). We then initialize the optimizer such that only the
              parameters of this newly initialized fully connected layer are optimized so that we don't finetune the weights of the previous layers that have been loaded from pretraining. AlexNet has 8 layers.
            </p>

            <h4><a href="https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html#resnet18" target="_blank">ResNet18</a></h4>
            
            <p>
              ResNet is another landmark CNN model that won the ImageNet challenge in 2015. It is the most cited neural network of the 21st century. The model was proposed by <a href="https://scholar.google.com/citations?user=DhtAFkwAAAAJ&hl=en" target="_blank">Kaiming He</a> and his colleagues.
              We load the pretrained version of this model and then replace the fully connected output layer with a newly initialized fully connected layer that has 5 output nodes (for the 5 rice varieties in the dataset). We then initialize the optimizer such that only the
              parameters of this newly initialized fully connected layer are optimized so that we don't finetune the weights of the previous layers that have been loaded from pretraining. There are many variants of ResNet such as ResNet18, ResNet34, ResNet50 and so on, but we chose to use the
              ResNet18 model as it has 18 layers and is the most comparable to AlexNet in terms of number of layers.
            </p>

            <h4><a href="https://pytorch.org/vision/main/_modules/torchvision/models/squeezenet.html" target="_blank">SqueezeNet (1.1)</a></h4>

            <p>
              SqueezeNet is a smaller CNN model that was designed as a more compact replacement for AlexNet. It has almost 50x fewer parameters, performs 3x faster and achieves comparable accuracy to AlexNet on the ImageNet dataset. SqueezeNet was developed by researchers at DeepScale, Stanford University,
              and the University of California, Berkeley. It was proposed in a paper called <a href="https://arxiv.org/abs/1602.07360" target="_blank">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt; 0.5MB model size</a>.
              We load the pretrained version of this model and then replace the last convolutional layer with a newly initialized convolutional layer that has 5 output nodes (for the 5 rice varieties in the dataset). We then initialize the optimizer such that only the
              parameters of this newly initialized convolutional layer are optimized so that we don't finetune the weights of the previous layers that have been loaded from pretraining.
            </p>

            <br/>

            <h3>Optimizers</h3>

            <p>
              For all the optimizers, we used the same initial learning rate of 0.01. For SGD and RMSprop, we also use a momentum value of 0.9 as is standard practice.
            </p>

            <h4><a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html" target="_blank">SGD</a></h4>

            <p>
              In contrast to regular gradient descent, mini-batch stochastic gradient descent (SGD) performs paramater updates for each batch of training examples rather than the entire training set. This reduces the variance of the parameter updates and generally leads to the best performance out of
              gradient descent variants.
            </p>

            <h4><a href="https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html" target="_blank">Adadelta</a></h4>

            <p>
              Adadelta is an extension of Adagrad. Adagrad tries to lower the learning rate for parameters associated with frequently occurring features and larger updates for infrequent features. Adadelta tries to reduce the 
              monotonically decreasing learning rate of Adagrad by restrciting the window of accumulated past gradients to a fixed size.
            </p>

            <h4><a href="https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html" target="_blank">RMSprop</a></h4>

            <p>
              RMSprop tries to solve the same issue of Adagrad that Adadelta does by resolving the rapidly diminishing learning rates. RMSprop additionally divides the learning rate by an exponentially decaying average of squared gradients.
            </p>

          </div>
        </div>

      </div>
    </section>

    <section id="results" class="about">
      <div class="container">

        <div class="row">
          <div class="col-lg-12 content">
            <h2>Results</h2>

            <p>
              Here are the final test accuracies for the models.
            </p>

            <p style="text-align:center;"><img src="assets/img/Test Accuracies.png"></p>

            <h3>AlexNet</h3>

            <p>Here are the loss and accuracy plots for AlexNet using the three different optimizers.</p>

            <h4>Loss plots:</h4>

            <p style="text-align:center;"><img src="assets/img/AlexNet SGD Loss.png"></p>
            <p style="text-align:center;"><img src="assets/img/AlexNet Adadelta Loss.png"></p>
            <p style="text-align:center;"><img src="assets/img/AlexNet RMSprop Loss.png"></p>

            <h4>Accuracy plots:</h4>

            <p style="text-align:center;"><img src="assets/img/AlexNet SGD Accuracy.png"></p>
            <p style="text-align:center;"><img src="assets/img/AlexNet Adadelta Accuracy.png"></p>
            <p style="text-align:center;"><img src="assets/img/AlexNet RMSprop Accuracy.png"></p>

            <br/>

            <h3>ResNet18</h3>

            <p>Here are the loss and accuracy plots for ResNet18 using the three different optimizers.</p>

            <h4>Loss plots:</h4>

            <p style="text-align:center;"><img src="assets/img/ResNet18 SGD Loss.png"></p>
            <p style="text-align:center;"><img src="assets/img/ResNet18 Adadelta Loss.png"></p>
            <p style="text-align:center;"><img src="assets/img/ResNet18 RMSprop Loss.png"></p>

            <h4>Accuracy plots:</h4>

            <p style="text-align:center;"><img src="assets/img/ResNet18 SGD Accuracy.png"></p>
            <p style="text-align:center;"><img src="assets/img/ResNet18 Adadelta Accuracy.png"></p>
            <p style="text-align:center;"><img src="assets/img/ResNet18 RMSprop Accuracy.png"></p>

            <br/>

            <h3>SqueezeNet</h3>

            <p>Here are the loss and accuracy plots for SqueezeNet using the three different optimizers.</p>

            <h4>Loss plots:</h4>

            <p style="text-align:center;"><img src="assets/img/SqueezeNet SGD Loss.png"></p>
            <p style="text-align:center;"><img src="assets/img/SqueezeNet Adadelta Loss.png"></p>
            <p style="text-align:center;"><img src="assets/img/SqueezeNet RMSprop Loss.png"></p>

            <h4>Accuracy plots:</h4>

            <p style="text-align:center;"><img src="assets/img/SqueezeNet SGD Accuracy.png"></p>
            <p style="text-align:center;"><img src="assets/img/SqueezeNet Adadelta Accuracy.png"></p>
            <p style="text-align:center;"><img src="assets/img/SqueezeNet RMSprop Accuracy.png"></p>
            
          </div>
        </div>

      </div>
    </section>

    <section id="discussion" class="about">
      <div class="container">

        <div class="row">
          <div class="col-lg-12 content">
            <h2>Discussion</h2>
            <ul>
              <li><i class="bi bi-check-circle"></i> Across the board, AlexNet is the best performing model and Adadelta is the best performing optimizer.</li>
              <li><i class="bi bi-check-circle"></i> SqueezeNet performs better than ResNet18 with SGD and Adadelta, but not with RMSprop.</li>
              <li><i class="bi bi-check-circle"></i> Perhaps the learning rate decay of RMSprop is too aggressive</li>
              <li><i class="bi bi-check-circle"></i> AlexNet has 61M parameters, ResNet18 has 11M parameters and SqueezeNet has 1M parameters.</li>
              <li><i class="bi bi-check-circle"></i> SqueezeNet with Adadelta and RMSprop loss and accuracy curves are not smooth.</li>
              <li><i class="bi bi-check-circle"></i> Perhaps lower number of parameters contributes to less stable learning when combined with learning rate decay.</li>
            </ul>
            
          </div>
        </div>

      </div>
    </section>

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">

    <div class="footer-top">
      <div class="container">
        <div class="row">

          <div class="col-lg-g col-md-6 footer-contact">
            <h3>Abhishek Babu</h3>
            <p>
              <strong>Email:</strong> <a href="mailto:babua@cs.washington.edu" target="_blank">babua@cs.washington.edu</a><br>
            </p>
          </div>

          <div class="col-lg-g col-md-6 footer-contact">
            <h3>Phuong Nguyen</h3>
            <p>
              <strong>Email:</strong> <a href="mailto:phdn@cs.washington.edu" target="_blank">phdn@cs.washington.edu</a><br>
            </p>
          </div>

        </div>
      </div>
    </div>

    <div class="container d-md-flex py-4">

      <div class="me-md-auto text-center text-md-start">
        <div class="credits">
        </div>
      </div>
      <div class="social-links text-center text-md-right pt-3 pt-md-0">
        <a href="https://www.kaggle.com/datasets/muratkokludataset/rice-image-dataset" target="_blank" class="data"><i class="bx bxs-data"></i></a>
        <a href="https://github.com/abhishekbabu/cse455-rice-classification" target="_blank" class="github"><i class="bx bxl-github"></i></a>
        <a href="https://youtu.be/DBiEtzIXmxc" target="_blank" class="video"><i class="bx bxs-video"></i></a>
      </div>
    </div>
  </footer><!-- End Footer -->

  <div id="preloader"></div>
  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>